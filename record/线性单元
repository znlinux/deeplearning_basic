	1.线性单元（解决回归问题：返回实数值）<br>
当面对的数据集不是线性可分的（分开后有交集），感知器规则可能无法收敛->无法完成训练<br>
解决：使用可导的线性函数来替代感知器的阶跃函数->线性单元（能收敛）<br>
y=h(x)=w^T x<br>
例子：（思想）在处理语音识别的时候，获取语音容易，但是一个个标注相关文本较为复杂，所以先用无监督方法做一些聚类，模型总结相似音节，用少量带标注的训练样本告诉模型这些音节对应的文字
	2.目标函数<br>
监督学习下：x表示特征，y表示实际值(label)，y ̅表示预测值<br>
其中y与 y ̅ 的接近程度，数学上有很多的方法：<br>
	如用e = 1/2(y - y ̅)^2 表示单个样本的误差<br>
用误差和表示模型的误差：<br>
E= ∑_(i=1)^n▒e^((i))  = 1/2 ∑_(i=1)^n▒〖(y^((i) )-y ̅^((i) ) )^2 〗= 1/2 ∑_(i=1)^n▒〖(y^((i) )-W^T x^((i)) )^2 〗<br>
由此可见，模型的训练=求取w使得式子取最小值<br>
	3.梯度下降优化算法<br>
对于计算机：可以凭借计算能力，一步步将函数的极值点试出来<br>
 <br>
任意选定一点，如x_0 ，每次迭代修改x的值<br>
梯度：指向函数数值上升最快的方向（函数定义：相对于各个变量的偏导数）<br>
梯度下降算法的公式：<br>
x_new=x_old- η∇f(x)<br>
∇f(x)是f(x)的梯度，η是步长即学习速率<br>
<br>
则结合上文的目标函数，梯度下降算法可写为<br>
w_new=w_old- η∇E(w)<br>
梯度上升算法为：<br>
w_new=w_old+ η∇E(w)<br>
<br>
进一步得到：<br> 
w_new=w_old+ η∑_(i=1)^n▒〖(y^((i) )-y ̅^((i) ))x^((i)) 〗<br>
批梯度下降，每次更新都要遍历训练数据中所有的样本进行计算<br>
	∇E(w)的推导：<br>
			∇E(w)=∂/∂w E(w)=  ∂/∂w  1/2 ∑_(i=1)^n▒〖(y^((i) )-y ̅^((i) ) )^2 〗<br>
<br>
	4.随机梯度下降算法(Stochastic Gradient Descent, SGD)<br>
批梯度下降算法样本非常大，SGD每次更新w的迭代，只计算一个样本。<br>
具有随机性<br>
思想（步骤）：<br>
	定义激活函数<br>
	初始化线性单元，设置输入参数的个数<br>
	输入样本<br>
	使用数据训练线性单元<br>
	执行训练并打印<br>
	测试<br>
5.	总结<br>
机器学习算法<br>：
	模型的预测函数<br>
	目标函数：目标函数取最小值或最大值所对应的参数值。<br>
算法的关键是#选取特征#。而神经网络的一个优势是能自动学习到应该提取什么特征，从而使算法不再依赖人类。<br>
