1. 神经网络：<br> 
 1.1.神经元： 与感知器本质一样，激活函数往往选择sigmoid函数或tanh函数<br>
	1.1.1. 针对激活函数是sigmoid(x)，y=sigmoid(w^T* x)<br>
  激活函数为<br>
                y= sigmoid(x)=  1/(1+ e^(-x) )<br>
<br>
   其导数是 <br>
                 y^'=y (1-y)<br>
    值域：<br>
                    (0~1)<br>
	1.1.2.针对tanh函数：是对sign函数的逼近或近似，是对sign离散性而导致的难以优化的缺陷的弥补<br>
             tanh⁡(x)=  ( exp⁡(x)-exp⁡(-x))/(exp⁡(x)+exp⁡(-x))<br>
    导数为：<br>
              tanh^' (x)=1-〖tanh〗^2 (x)<br>
    值域：<br>
                (-1~1)<br>
	 1.1.3.Relu函数（Rectified Linear Units）<br>
	<br>
            relu(x)=max⁡(0,z)<br>

			值域：<br>
         (0~+∞)<br>
 <br>
 
	1.1.4. 目前，Relu函数在神经网络模型研究和实际应用中应用较多，因为<br>
  ①． 使用sigmoid或tanh作为激活函数做监督学习时，会遇到梯度消失问题导致无法收敛<br>
  ②． RELU的计算开销相对较小<br>
	1.1.5.  激活函数：<br>
    作用：<br>
		  从原始特征学习出新特征，或者是或将原始特征从低维空间映射到高维空间；<br>
      引入激活函数是神经网络具有优异性能的关键所在，多层级联的结构加上激活函数，另多层神经网络可以逼近任意函数，从而学习出非常复杂的假设函数<br>
